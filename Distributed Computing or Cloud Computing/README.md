# Material that I have looked into
1. [GCP in Chinese](https://github.com/tinghe14/apachecn-dl-zh/blob/master/docs/handson-ai-gcp/SUMMARY.md): not clear, waste of time
2. official GCP repo: clear but the docker image part has been outdated
    - [Github repo](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/main): include information about project setup, docker image creation and sample code of pytorch,BUT container part is too old
    - pytorch code sample using published container and built-in dataset, [tutorial](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/main/pytorch/containers/published_container): container/docker image, can think as conda environment locally
    - pytorch code sample using custom container and built-in dataset,[tutorial](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/main/pytorch/containers/custom_container): additional docker image creation file
    - pytorch code sample using custom container with hyperparameter tuning and built-in dataset,[tutorial](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/main/pytorch/containers/hp_tuning): additional hyperparameter tuning
3. official GCP tutorial: updated but It can't follow
    - [container part](https://cloud.google.com/ai-platform/docs/getting-started-keras)
    - [Training PyTorch Transformers on Google Cloud AI Platform](https://nordcloud.com/tech-community/training-pytorch-transformers-on-google-cloud-ai-platform/): include [best practices](https://cloud.google.com/ai-platform/training/docs/packaging-trainer) to packing code to GCP, and [recommended project structure](https://cloud.google.com/ai-platform/training/docs/packaging-trainer#project-structure)
4. official Vertex AI tutorial: most updated and clear to follow
    - [Youtube, overview of documentation](https://www.youtube.com/watch?v=VRQXIiNLdAk): use pretrained docker image, create a docker file, and share the skeleton of image file; previous episode contains how to upload local data to cloud storage
        - tutorial - [steps by steps](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbkRaWElYeVhSdnNuZS1uUENYZHlqQ1VHeGJtZ3xBQ3Jtc0tremd0WGw0X1VxcTlYMzhfQzg4bkpxaDZfN25jSFZHUDg0bDZyUzRKdHB6bk5VbzYzRkJkQ0RGR1FqcHFOanMzVWdJdS05bF9UNlFsUUlCR0lRTVpqd3ZZQUpvUjRyZnRIV2tmaHVNM3hwNWt1UzFUVQ&q=https%3A%2F%2Fgoo.gle%2F3w7kGvV&v=VRQXIiNLdAk): very useful and clear! pay attention to some linked documentations.(1) setup GCP environment: install the Google Cloud SDK, authenticate and set your project (2)prepare your docker container:create a new directory for your containerization project. in this directory, create a new file called 'Dockerfile', add required content (3) package the project files (4) build and tag the docker image (5) pusch the docer image to a container registry (6) deploy and run the container on GCP
            - linked documentation - [base docker image list](https://cloud.google.com/deep-learning-containers/docs/choosing-container#choose_a_container_image_type?utm_campaign=CDR_sar_aiml_ucaiplabs_011321&utm_source=external&utm_medium=web): create docker file and if we want to use predefined image, this is what should looking for at FROM statement
            - linked documentation - [more about how to create a docker file](https://cloud.google.com/vertex-ai/docs/training/create-custom-container#create_a_dockerfile): include not only FROM statement
            - linked documentation - [docker repo which create in the begining if no step was missing](https://cloud.google.com/artifact-registry/docs/docker/store-docker-container-images#before-you-begin): REPO_NAME statement
            - linked documentation - [Define a variable with the URI of your container image in Google Artifact Registry](https://cloud.google.com/vertex-ai/docs/training/create-custom-container#create_a_dockerfile): use this step here instead of steps by steps tutorial. This is more clear.
            - error message I had - [Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?](https://stackoverflow.com/questions/54437744/how-to-start-docker-from-command-line-in-mac): solution to open a docker
            - previous code sample - [GCP repo](https://github.com/GoogleCloudPlatform/cloudml-samples/tree/main/pytorch/containers/published_container): used the tutorial in example 2 followed the part 2 and part 3 code, it can build a docker image and teach how to run locally first
            - error message I had - [WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested](https://stackoverflow.com/questions/72152446/warning-the-requested-images-platform-linux-amd64-does-not-match-the-detecte#:~:text=Put%20this%20line%20%2D%2Dplatform%20linux/amd64%20after%20docker%20run.%20It%20works%20for%20me%2C%20using%20Macbook%20M1.)
            - error message I had - [/bin/sh: 1: python,: not found](https://stackoverflow.com/questions/32709075/docker-image-error-bin-sh-1-python-not-found)
            - error message I had - [file can't find]: the file directory should be /gcs/my bucket name/my directory name not gs:// -> wrong directin
            - more about file can't find: [Mount a Cloud Storage bucket using Cloud Storage FUSE](https://cloud.google.com/storage/docs/gcsfuse-quickstart-mount-bucket):Learn how to mount a Cloud Storage bucket as a local filesystem using Cloud Storage FUSE, so you can interact with your objects using standard file system semantics.
                - error message I had - [Install Cloud Storage FUSE](): previous tutorial only work for linux not mac
                - tutorial - [Recommended Training Application Structure for Vertex AI and GCS FUSE](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/community-content/pytorch_text_classification_using_vertex_sdk_and_gcloud/pytorch-text-classification-vertex-ai-train-tune-deploy.ipynb)
                - tutorial - [Install Cloud Storage FUSE in mac](https://cloud.google.com/storage/docs/gcsfuse-install#install-source-code), the [medium post](https://medium.com/analytics-vidhya/improve-workflow-with-cloud-storage-fuse-89b8d76d0886) that is helpful
                    - stack overflow - [pass Google Cloud Platform's service account JSON credentials file to a docker container so that the container can access a cloud storage bucket](https://stackoverflow.com/questions/72341611/how-to-pass-google-cloud-application-credentials-file-to-docker-container)
                    - stack overflow - [debug code](https://stackoverflow.com/questions/76169800/gcsfuse-not-running-command-with-set-service-account): after that I know that my personal account don't have access to google storage, then I assigned it via console
    - tutorial - [setup](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)
    - tutorial - [export environment at GCI](https://cloud.google.com/vertex-ai/docs/tutorials/text-classification-automl)
    - tutorial - [Create a Python training application for a prebuilt container](https://cloud.google.com/vertex-ai/docs/training/create-python-pre-built-container?_ga=2.78161364.1169932329.1685983386-1703019298.1684697406&_gac=1.123091193.1685635930.Cj0KCQjw4NujBhC5ARIsAF4Iv6dUxaJlcW7I1ourBHTusyrNz8FSF2JwF3IkOTpaH20BRe0oxoX7LkUaAremEALw_wcB)
        - tutorial - [Containerize and run training code locally](https://cloud.google.com/vertex-ai/docs/training/containerize-run-code-local)
        - tutorial - [Push the container to Artifact Registry](https://cloud.google.com/vertex-ai/docs/training/create-custom-container#build-and-push-container)
